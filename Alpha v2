{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14919093,"datasetId":9546024,"databundleVersionId":15785529},{"sourceType":"datasetVersion","sourceId":14917655,"datasetId":9545058,"databundleVersionId":15783936}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nAlpha v2 Streaming Dataset Builder\nHard capped at exactly 7M tokens\nMemory safe (streaming=True)\n\"\"\"\n\nimport random\nimport json\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport tiktoken\n\n# ==============================\n# CONFIG\n# ==============================\n\nTARGET_TOTAL_TOKENS = 7_000_000\n\nSPLIT_CONFIG = {\n    \"instruction\": 0.40,\n    \"reasoning\": 0.20,\n    \"code\": 0.15,\n    \"windows\": 0.15,\n    \"clean_text\": 0.10\n}\n\nWINDOWS_FILE_PATH = \"/kaggle/input/datasets/majorlakshya/windows/windows.jsonl\"\nOUTPUT_FILE = \"alpha_v2_7M.jsonl\"\n\nenc = tiktoken.get_encoding(\"gpt2\")\n\ndef count_tokens(text):\n    return len(enc.encode(text))\n\n# ==============================\n# STREAM COLLECTOR\n# ==============================\n\ndef stream_collect(dataset, text_fn, token_budget):\n    selected = []\n    total = 0\n\n    for sample in dataset:\n        text = text_fn(sample)\n        if not text:\n            continue\n\n        tokens = count_tokens(text)\n\n        if total + tokens > token_budget:\n            break\n\n        selected.append(text)\n        total += tokens\n\n    return selected, total\n\n# ==============================\n# WINDOWS LOADER (LOCAL)\n# ==============================\n\ndef stream_windows(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            yield json.loads(line)\n\n# ==============================\n# MAIN\n# ==============================\n\ndef main():\n\n    final_dataset = []\n    total_tokens = 0\n\n    for category, pct in SPLIT_CONFIG.items():\n\n        category_budget = int(TARGET_TOTAL_TOKENS * pct)\n        remaining_global = TARGET_TOTAL_TOKENS - total_tokens\n        category_budget = min(category_budget, remaining_global)\n\n        print(f\"\\nBuilding {category} â†’ Target {category_budget}\")\n\n        if category == \"instruction\":\n\n            alpaca = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", streaming=True)\n            dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\", streaming=True)\n\n            data_iter = (\n                (f\"User: {x['instruction']}\\nAssistant: {x['output']}\" for x in alpaca)\n            )\n            collected1, used1 = stream_collect(data_iter, lambda x: x, category_budget)\n\n            remaining = category_budget - used1\n\n            data_iter2 = (\n                (f\"User: {x['instruction']}\\nAssistant: {x['response']}\" for x in dolly)\n            )\n            collected2, used2 = stream_collect(data_iter2, lambda x: x, remaining)\n\n            final_dataset.extend(collected1 + collected2)\n            total_tokens += used1 + used2\n\n        elif category == \"reasoning\":\n\n            gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"train\", streaming=True)\n\n            data_iter = (\n                (f\"Question: {x['question']}\\nAnswer: {x['answer']}\" for x in gsm8k)\n            )\n\n            collected, used = stream_collect(data_iter, lambda x: x, category_budget)\n\n            final_dataset.extend(collected)\n            total_tokens += used\n\n        elif category == \"code\":\n\n            code_ds = load_dataset(\n                \"Programming-Language/codeagent-python\",\n                split=\"train\",\n                streaming=True\n            )\n\n            data_iter = (\n                json.dumps({\n                    \"instruction\": x.get(\"instruction\", \"\"),\n                    \"input\": x.get(\"input\", \"\"),\n                    \"output\": x.get(\"output\", \"\")\n                })\n                for x in code_ds\n            )\n\n            collected, used = stream_collect(data_iter, lambda x: x, category_budget)\n\n            final_dataset.extend(collected)\n            total_tokens += used\n\n        elif category == \"windows\":\n\n            data_iter = (\n                json.dumps(x, ensure_ascii=False)\n                for x in stream_windows(WINDOWS_FILE_PATH)\n            )\n\n            collected, used = stream_collect(data_iter, lambda x: x, category_budget)\n\n            final_dataset.extend(collected)\n            total_tokens += used\n\n        elif category == \"clean_text\":\n\n            openweb = load_dataset(\n                \"Skylion007/openwebtext\",\n                split=\"train\",\n                streaming=True\n            )\n\n            data_iter = (x[\"text\"] for x in openweb)\n\n            collected, used = stream_collect(data_iter, lambda x: x, category_budget)\n\n            final_dataset.extend(collected)\n            total_tokens += used\n\n        print(f\"Running total: {total_tokens}\")\n\n        if total_tokens >= TARGET_TOTAL_TOKENS:\n            break\n\n    # ==============================\n    # TOP-UP IF UNDERFILLED\n    # ==============================\n\n    if total_tokens < TARGET_TOTAL_TOKENS:\n\n        deficit = TARGET_TOTAL_TOKENS - total_tokens\n        print(f\"\\nTop-up required: {deficit}\")\n\n        openweb = load_dataset(\n            \"Skylion007/openwebtext\",\n            split=\"train\",\n            streaming=True\n        )\n\n        data_iter = (x[\"text\"] for x in openweb)\n\n        collected, used = stream_collect(data_iter, lambda x: x, deficit)\n\n        final_dataset.extend(collected)\n        total_tokens += used\n\n    print(f\"\\nFinal token count before cap: {total_tokens}\")\n\n    # ==============================\n    # HARD CAP\n    # ==============================\n\n    random.shuffle(final_dataset)\n\n    capped = []\n    final_count = 0\n\n    for text in final_dataset:\n        tokens = count_tokens(text)\n\n        if final_count + tokens > TARGET_TOTAL_TOKENS:\n            break\n\n        capped.append(text)\n        final_count += tokens\n\n    print(f\"\\nFINAL TOKEN COUNT: {final_count}\")\n\n    # ==============================\n    # SAVE\n    # ==============================\n\n    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n        for text in capped:\n            f.write(text.strip() + \"\\n\")\n\n    print(f\"\\nSaved dataset to {OUTPUT_FILE}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pickle\n\nimport random\n\nINPUT_FILE = \"alpha_v2_7M.jsonl\"\nOUTPUT_FILE = \"alpha_v2_7M_Shuffled.jsonl\"\n\nwith open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n    lines = f.readlines()\nprint(f\"Loaded {len(lines)} lines\")\nrandom.shuffle(lines)\n\nwith open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n    f.writelines(lines)\n\nprint(\"Shuffled file saved.\")\n\ndata = []\nwith open('alpha_v2_7M.jsonl', 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nwith open('alpha_v2_7M.bin', 'wb') as f:\n    pickle.dump(data, f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ntokens = np.fromfile(\"alpha_v2_7M.bin\", dtype=np.uint16)\n\nsplit = int(len(tokens) * 0.9)\n\ntokens[:split].tofile(\"train.bin\")\ntokens[split:].tofile(\"val.bin\")\n\nprint(\"Train/Val split complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nBIN_FILE = \"alpha_v2_7M.bin\"\nDTYPE = np.uint16  # must match how you saved it\n\ntokens = np.memmap(BIN_FILE, dtype=DTYPE, mode=\"r\")\n\nprint(\"Total tokens:\", len(tokens))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n#block size = context window\n\ndef get_batch(split):\n    # We recreate np.memmap every batch to avoid a memory leak, as per\n    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n    if split == 'train':\n        data = np.memmap(\"/kaggle/input/datasets/majorlakshya/alpha-v2-dataset/Alpha_v2_7M_train.bin\", dtype=np.uint16, mode='r')\n    else:\n        data = np.memmap(\"/kaggle/input/datasets/majorlakshya/alpha-v2-dataset/Alpha_v2_7M_val.bin\", dtype=np.uint16, mode='r')\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if device_type == 'cuda':\n        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T15:33:19.745198Z","iopub.execute_input":"2026-02-22T15:33:19.745518Z","iopub.status.idle":"2026-02-22T15:33:19.751725Z","shell.execute_reply.started":"2026-02-22T15:33:19.745492Z","shell.execute_reply":"2026-02-22T15:33:19.750837Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"config = GPTConfig(\n    vocab_size=50257,     # use the tokenizer's vocab size\n    block_size=256,       # or whatever context size you're training with\n    n_layer = 12,\n    n_head  = 12,\n    n_embd  = 768,\n    dropout = 0.1,\n    bias=True\n)\n\nmodel = GPT(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T15:33:16.056541Z","iopub.execute_input":"2026-02-22T15:33:16.057013Z","iopub.status.idle":"2026-02-22T15:33:16.125465Z","shell.execute_reply.started":"2026-02-22T15:33:16.056971Z","shell.execute_reply":"2026-02-22T15:33:16.124911Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom dataclasses import dataclass\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom contextlib import nullcontext\nimport os\n\nclass LayerNorm(nn.Module):\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n    def forward(self, x):\n        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.flash = hasattr(F, 'scaled_dot_product_attention')\n        if not self.flash:\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                       .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n        if self.flash:\n            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n    def forward(self, x):\n        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln1 = LayerNorm(config.n_embd, config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = LayerNorm(config.n_embd, config.bias)\n        self.mlp = MLP(config)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int\n    vocab_size: int\n    n_layer: int\n    n_head: int\n    n_embd: int\n    dropout: float = 0.0\n    bias: bool = True\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte=nn.Embedding(config.vocab_size, config.n_embd),\n            wpe=nn.Embedding(config.block_size, config.n_embd),\n            drop=nn.Dropout(config.dropout),\n            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f=LayerNorm(config.n_embd, config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size\n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n\n        tok_emb = self.transformer.wte(idx)\n        pos_emb = self.transformer.wpe(pos)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n            return logits, loss\n        else:\n            logits = self.lm_head(x[:, [-1], :])\n            return logits, None\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Generate tokens given a conditioning sequence.\n        idx: Tensor of shape (B, T)\n        \"\"\"\n        for _ in range(max_new_tokens):\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T15:33:12.552383Z","iopub.execute_input":"2026-02-22T15:33:12.553004Z","iopub.status.idle":"2026-02-22T15:33:12.578188Z","shell.execute_reply.started":"2026-02-22T15:33:12.552979Z","shell.execute_reply":"2026-02-22T15:33:12.577523Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def estimate_loss(model):\n    out = {}\n    model.eval()\n    with torch.inference_mode():\n        for split in ['train', 'val']:\n            losses = torch.zeros(eval_iters)\n            for k in range(eval_iters):\n                X, Y = get_batch(split)\n                with ctx:\n                    logits, loss = model(X, Y)\n                losses[k] = loss.item()\n            out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T15:33:03.036329Z","iopub.execute_input":"2026-02-22T15:33:03.036828Z","iopub.status.idle":"2026-02-22T15:33:03.042075Z","shell.execute_reply.started":"2026-02-22T15:33:03.036800Z","shell.execute_reply":"2026-02-22T15:33:03.041299Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Training Config\nimport torch\nfrom contextlib import nullcontext\n\nlearning_rate = 3e-4 #more stable training, earlier 1e-4\nmax_iters = 30000 #increase from 25000\nwarmup_steps = 2000 #2000 #smoother initial train, earlier 100\nmin_lr = 3e-5 #lower rate, earlier 5e-4\neval_iters = 250 # increased from 100\nbatch_size = 8 # changed from 16, better gradient estimate\nblock_size = 256 #changed from 64, capture longer range dependencies\n\ngradient_accumulation_steps = 128 # reduced from 50\n\ndevice =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n# note: float16 data type will automatically use a GradScaler\n\n# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\ntorch.set_default_device(device)\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T15:32:57.585597Z","iopub.execute_input":"2026-02-22T15:32:57.585883Z","iopub.status.idle":"2026-02-22T15:32:59.666742Z","shell.execute_reply.started":"2026-02-22T15:32:57.585859Z","shell.execute_reply":"2026-02-22T15:32:59.665907Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7e6c0f4db210>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom tqdm.auto import tqdm\n\n\nresume_path = \"/kaggle/working/Saved_At2000.pt\"  \n\nstart_epoch = 2000\nbest_val_loss = float(\"inf\")\ntrain_loss_list = []\nvalidation_loss_list = []\n\nprint(torch.cuda.current_device())\nprint(torch.cuda.get_device_name())\n\nif os.path.exists(resume_path):\n    print(\"Loading checkpoint:\", resume_path)\n\n    checkpoint = torch.load(resume_path, map_location=device)\n\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    \n    for state in optimizer.state.values():\n        for k, v in state.items():\n            if torch.is_tensor(v):\n                state[k] = v.to(device)\n\n    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n    scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n\n    best_val_loss = checkpoint[\"best_val_loss\"]\n\n    # load loss history if exists\n    if \"train_loss_list\" in checkpoint:\n        train_loss_list = checkpoint[\"train_loss_list\"]\n        validation_loss_list = checkpoint[\"validation_loss_list\"]\n\n    print(f\"Resuming from epoch {start_epoch}\")\n\nelse:\n    print(\"No checkpoint found. Starting fresh.\")\n\n\nmodel = model.to(device)\n\nprogress_bar = tqdm(range(start_epoch, max_iters), desc=\"Training\")\n\nfor epoch in progress_bar:\n\n    # --------------------\n    # TRAIN STEP\n    # --------------------\n    X, y = get_batch(\"train\")\n    X, y = X.to(device), y.to(device)\n\n    with ctx:\n        logits, loss = model(X, y)\n        loss = loss / gradient_accumulation_steps\n        scaler.scale(loss).backward()\n\n    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad(set_to_none=True)\n\n    scheduler.step()\n\n    # --------------------\n    # EVAL EVERY 100\n    # --------------------\n    if (epoch + 1) % 250 == 0:\n\n        losses = estimate_loss(model)\n\n        train_loss = losses[\"train\"].item()\n        val_loss   = losses[\"val\"].item()\n        current_lr = optimizer.param_groups[0][\"lr\"]\n\n        train_loss_list.append(train_loss)\n        validation_loss_list.append(val_loss)\n\n        # Update tqdm postfix\n        progress_bar.set_postfix(\n            train=f\"{train_loss:.4f}\",\n            val=f\"{val_loss:.4f}\",\n            lr=f\"{current_lr:.6f}\"\n        )\n\n        # Print stacked logs BELOW progress bar\n        tqdm.write(f\"Epoch {epoch+1}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n        tqdm.write(f\"The current learning rate: {current_lr:.5f}\")\n\n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model_params.pt\")\n\n    # --------------------\n    # SAVE EVERY 1000\n    # --------------------\n    if (epoch + 1) % 1000 == 0:\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n                \"scheduler_state_dict\": scheduler.state_dict(),\n                \"scaler_state_dict\": scaler.state_dict(),\n                \"best_val_loss\": best_val_loss,\n                \"train_loss_list\": train_loss_list,\n                \"validation_loss_list\": validation_loss_list,\n            },\n            f\"Saved_At{epoch+1}.pt\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T15:33:32.191066Z","iopub.execute_input":"2026-02-22T15:33:32.191822Z","iopub.status.idle":"2026-02-22T17:53:11.282884Z","shell.execute_reply.started":"2026-02-22T15:33:32.191790Z","shell.execute_reply":"2026-02-22T17:53:11.281651Z"}},"outputs":[{"name":"stdout","text":"0\nTesla T4\nLoading checkpoint: /kaggle/working/Saved_At2000.pt\nResuming from epoch 2000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/28000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f83840fe51645a09abf23a36a57e9a5"}},"metadata":{}},{"name":"stdout","text":"Epoch 2250: train loss 3.9720, val loss 4.0030\nThe current learning rate: 0.00017\nEpoch 2500: train loss 3.9090, val loss 3.9482\nThe current learning rate: 0.00018\nEpoch 2750: train loss 3.8724, val loss 3.8768\nThe current learning rate: 0.00018\nEpoch 3000: train loss 3.8276, val loss 3.8220\nThe current learning rate: 0.00019\nEpoch 3250: train loss 3.7913, val loss 3.7670\nThe current learning rate: 0.00020\nEpoch 3500: train loss 3.7420, val loss 3.7656\nThe current learning rate: 0.00021\nEpoch 3750: train loss 3.6869, val loss 3.7299\nThe current learning rate: 0.00022\nEpoch 4000: train loss 3.6552, val loss 3.6967\nThe current learning rate: 0.00023\nEpoch 4250: train loss 3.6128, val loss 3.6426\nThe current learning rate: 0.00025\nEpoch 4500: train loss 3.5910, val loss 3.6229\nThe current learning rate: 0.00026\nEpoch 4750: train loss 3.5323, val loss 3.6039\nThe current learning rate: 0.00027\nEpoch 5000: train loss 3.5054, val loss 3.5553\nThe current learning rate: 0.00028\nEpoch 5250: train loss 3.5194, val loss 3.5352\nThe current learning rate: 0.00029\nEpoch 5500: train loss 3.4839, val loss 3.5188\nThe current learning rate: 0.00030\nEpoch 5750: train loss 3.4755, val loss 3.4927\nThe current learning rate: 0.00031\nEpoch 6000: train loss 3.4288, val loss 3.4754\nThe current learning rate: 0.00032\nEpoch 6250: train loss 3.4343, val loss 3.4921\nThe current learning rate: 0.00033\nEpoch 6500: train loss 3.3636, val loss 3.4390\nThe current learning rate: 0.00035\nEpoch 6750: train loss 3.3785, val loss 3.4562\nThe current learning rate: 0.00036\nEpoch 7000: train loss 3.3154, val loss 3.4343\nThe current learning rate: 0.00037\nEpoch 7250: train loss 3.3288, val loss 3.4000\nThe current learning rate: 0.00038\nEpoch 7500: train loss 3.3093, val loss 3.3633\nThe current learning rate: 0.00039\nEpoch 7750: train loss 3.3061, val loss 3.3661\nThe current learning rate: 0.00040\nEpoch 8000: train loss 3.2834, val loss 3.3493\nThe current learning rate: 0.00041\nEpoch 8250: train loss 3.2659, val loss 3.3431\nThe current learning rate: 0.00042\nEpoch 8500: train loss 3.2698, val loss 3.3053\nThe current learning rate: 0.00043\nEpoch 8750: train loss 3.2508, val loss 3.3153\nThe current learning rate: 0.00043\nEpoch 9000: train loss 3.1871, val loss 3.3247\nThe current learning rate: 0.00044\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_145/1943173861.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \"\"\"\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             return handle_torch_function(\n\u001b[0m\u001b[1;32m    617\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0;31m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_pop_mode_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1728\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n\n##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\noptimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n\nscheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\nscheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\nscheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n\n# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T15:33:25.536736Z","iopub.execute_input":"2026-02-22T15:33:25.537393Z","iopub.status.idle":"2026-02-22T15:33:26.575647Z","shell.execute_reply.started":"2026-02-22T15:33:25.537367Z","shell.execute_reply":"2026-02-22T15:33:26.574935Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_145/2132813893.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":7}]}