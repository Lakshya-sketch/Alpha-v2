{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14919093,"datasetId":9546024,"databundleVersionId":15785529},{"sourceType":"datasetVersion","sourceId":14917655,"datasetId":9545058,"databundleVersionId":15783936}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nAlpha v2 Streaming Dataset Builder\nHard capped at exactly 7M tokens\nMemory safe (streaming=True)\n\"\"\"\n\nimport random\nimport json\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport tiktoken\n\n# ==============================\n# CONFIG\n# ==============================\n\nTARGET_TOTAL_TOKENS = 7_000_000\n\nSPLIT_CONFIG = {\n    \"instruction\": 0.40,\n    \"reasoning\": 0.20,\n    \"code\": 0.15,\n    \"windows\": 0.15,\n    \"clean_text\": 0.10\n}\n\nWINDOWS_FILE_PATH = \"/kaggle/input/datasets/majorlakshya/windows/windows.jsonl\"\nOUTPUT_FILE = \"alpha_v2_7M.jsonl\"\n\nenc = tiktoken.get_encoding(\"gpt2\")\n\ndef count_tokens(text):\n    return len(enc.encode(text))\n\n# ==============================\n# STREAM COLLECTOR\n# ==============================\n\ndef stream_collect(dataset, text_fn, token_budget):\n    selected = []\n    total = 0\n\n    for sample in dataset:\n        text = text_fn(sample)\n        if not text:\n            continue\n\n        tokens = count_tokens(text)\n\n        if total + tokens > token_budget:\n            break\n\n        selected.append(text)\n        total += tokens\n\n    return selected, total\n\n# ==============================\n# WINDOWS LOADER (LOCAL)\n# ==============================\n\ndef stream_windows(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            yield json.loads(line)\n\n# ==============================\n# MAIN\n# ==============================\n\ndef main():\n\n    final_dataset = []\n    total_tokens = 0\n\n    for category, pct in SPLIT_CONFIG.items():\n\n        category_budget = int(TARGET_TOTAL_TOKENS * pct)\n        remaining_global = TARGET_TOTAL_TOKENS - total_tokens\n        category_budget = min(category_budget, remaining_global)\n\n        print(f\"\\nBuilding {category} → Target {category_budget}\")\n\n        if category == \"instruction\":\n\n            alpaca = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", streaming=True)\n            dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\", streaming=True)\n\n            data_iter = (\n                (f\"User: {x['instruction']}\\nAssistant: {x['output']}\" for x in alpaca)\n            )\n            collected1, used1 = stream_collect(data_iter, lambda x: x, category_budget)\n\n            remaining = category_budget - used1\n\n            data_iter2 = (\n                (f\"User: {x['instruction']}\\nAssistant: {x['response']}\" for x in dolly)\n            )\n            collected2, used2 = stream_collect(data_iter2, lambda x: x, remaining)\n\n            final_dataset.extend(collected1 + collected2)\n            total_tokens += used1 + used2\n\n        elif category == \"reasoning\":\n\n            gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"train\", streaming=True)\n\n            data_iter = (\n                (f\"Question: {x['question']}\\nAnswer: {x['answer']}\" for x in gsm8k)\n            )\n\n            collected, used = stream_collect(data_iter, lambda x: x, category_budget)\n\n            final_dataset.extend(collected)\n            total_tokens += used\n\n        elif category == \"code\":\n\n            code_ds = load_dataset(\n                \"Programming-Language/codeagent-python\",\n                split=\"train\",\n                streaming=True\n            )\n\n            data_iter = (\n                json.dumps({\n                    \"instruction\": x.get(\"instruction\", \"\"),\n                    \"input\": x.get(\"input\", \"\"),\n                    \"output\": x.get(\"output\", \"\")\n                })\n                for x in code_ds\n            )\n\n            collected, used = stream_collect(data_iter, lambda x: x, category_budget)\n\n            final_dataset.extend(collected)\n            total_tokens += used\n\n        elif category == \"windows\":\n\n            data_iter = (\n                json.dumps(x, ensure_ascii=False)\n                for x in stream_windows(WINDOWS_FILE_PATH)\n            )\n\n            collected, used = stream_collect(data_iter, lambda x: x, category_budget)\n\n            final_dataset.extend(collected)\n            total_tokens += used\n\n        elif category == \"clean_text\":\n\n            openweb = load_dataset(\n                \"Skylion007/openwebtext\",\n                split=\"train\",\n                streaming=True\n            )\n\n            data_iter = (x[\"text\"] for x in openweb)\n\n            collected, used = stream_collect(data_iter, lambda x: x, category_budget)\n\n            final_dataset.extend(collected)\n            total_tokens += used\n\n        print(f\"Running total: {total_tokens}\")\n\n        if total_tokens >= TARGET_TOTAL_TOKENS:\n            break\n\n    # ==============================\n    # TOP-UP IF UNDERFILLED\n    # ==============================\n\n    if total_tokens < TARGET_TOTAL_TOKENS:\n\n        deficit = TARGET_TOTAL_TOKENS - total_tokens\n        print(f\"\\nTop-up required: {deficit}\")\n\n        openweb = load_dataset(\n            \"Skylion007/openwebtext\",\n            split=\"train\",\n            streaming=True\n        )\n\n        data_iter = (x[\"text\"] for x in openweb)\n\n        collected, used = stream_collect(data_iter, lambda x: x, deficit)\n\n        final_dataset.extend(collected)\n        total_tokens += used\n\n    print(f\"\\nFinal token count before cap: {total_tokens}\")\n\n    # ==============================\n    # HARD CAP\n    # ==============================\n\n    random.shuffle(final_dataset)\n\n    capped = []\n    final_count = 0\n\n    for text in final_dataset:\n        tokens = count_tokens(text)\n\n        if final_count + tokens > TARGET_TOTAL_TOKENS:\n            break\n\n        capped.append(text)\n        final_count += tokens\n\n    print(f\"\\nFINAL TOKEN COUNT: {final_count}\")\n\n    # ==============================\n    # SAVE\n    # ==============================\n\n    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n        for text in capped:\n            f.write(text.strip() + \"\\n\")\n\n    print(f\"\\nSaved dataset to {OUTPUT_FILE}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pickle\n\nimport random\n\nINPUT_FILE = \"alpha_v2_7M.jsonl\"\nOUTPUT_FILE = \"alpha_v2_7M_Shuffled.jsonl\"\n\nwith open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n    lines = f.readlines()\nprint(f\"Loaded {len(lines)} lines\")\nrandom.shuffle(lines)\n\nwith open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n    f.writelines(lines)\n\nprint(\"Shuffled file saved.\")\n\ndata = []\nwith open('alpha_v2_7M.jsonl', 'r', encoding='utf-8') as f:\n    for line in f:\n        data.append(json.loads(line))\n\nwith open('alpha_v2_7M.bin', 'wb') as f:\n    pickle.dump(data, f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ntokens = np.fromfile(\"alpha_v2_7M.bin\", dtype=np.uint16)\n\nsplit = int(len(tokens) * 0.9)\n\ntokens[:split].tofile(\"train.bin\")\ntokens[split:].tofile(\"val.bin\")\n\nprint(\"Train/Val split complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nBIN_FILE = \"alpha_v2_7M.bin\"\nDTYPE = np.uint16  # must match how you saved it\n\ntokens = np.memmap(BIN_FILE, dtype=DTYPE, mode=\"r\")\n\nprint(\"Total tokens:\", len(tokens))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n#block size = context window\n\ndef get_batch(split):\n    # We recreate np.memmap every batch to avoid a memory leak, as per\n    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n    if split == 'train':\n        data = np.memmap(\"/kaggle/input/datasets/majorlakshya/alpha-v2-new-dataset-eos/alpha_v2_new_dataset_train.bin\", dtype=np.uint16, mode='r')\n    else:\n        data = np.memmap(\"/kaggle/input/datasets/majorlakshya/alpha-v2-new-dataset-eos/alpha_v2_new_dataset_val.bin\", dtype=np.uint16, mode='r')\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if device_type == 'cuda':\n        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T16:03:42.681504Z","iopub.execute_input":"2026-02-23T16:03:42.681827Z","iopub.status.idle":"2026-02-23T16:03:42.688599Z","shell.execute_reply.started":"2026-02-23T16:03:42.681798Z","shell.execute_reply":"2026-02-23T16:03:42.687679Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"config = GPTConfig(\n    vocab_size=50257,     # use the tokenizer's vocab size\n    block_size=256,       # or whatever context size you're training with\n    n_layer = 12,\n    n_head  = 12,\n    n_embd  = 768,\n    dropout = 0.1,\n    bias=True\n)\n\nmodel = GPT(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T16:03:45.209500Z","iopub.execute_input":"2026-02-23T16:03:45.209797Z","iopub.status.idle":"2026-02-23T16:03:45.230940Z","shell.execute_reply.started":"2026-02-23T16:03:45.209770Z","shell.execute_reply":"2026-02-23T16:03:45.230414Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom dataclasses import dataclass\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom contextlib import nullcontext\nimport os\n\nclass LayerNorm(nn.Module):\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n    def forward(self, x):\n        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.flash = hasattr(F, 'scaled_dot_product_attention')\n        if not self.flash:\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                       .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n        if self.flash:\n            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n    def forward(self, x):\n        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln1 = LayerNorm(config.n_embd, config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = LayerNorm(config.n_embd, config.bias)\n        self.mlp = MLP(config)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int\n    vocab_size: int\n    n_layer: int\n    n_head: int\n    n_embd: int\n    dropout: float = 0.0\n    bias: bool = True\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte=nn.Embedding(config.vocab_size, config.n_embd),\n            wpe=nn.Embedding(config.block_size, config.n_embd),\n            drop=nn.Dropout(config.dropout),\n            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f=LayerNorm(config.n_embd, config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size\n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n\n        tok_emb = self.transformer.wte(idx)\n        pos_emb = self.transformer.wpe(pos)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n            return logits, loss\n        else:\n            logits = self.lm_head(x[:, [-1], :])\n            return logits, None\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Generate tokens given a conditioning sequence.\n        idx: Tensor of shape (B, T)\n        \"\"\"\n        for _ in range(max_new_tokens):\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T16:03:47.181426Z","iopub.execute_input":"2026-02-23T16:03:47.182260Z","iopub.status.idle":"2026-02-23T16:03:47.208194Z","shell.execute_reply.started":"2026-02-23T16:03:47.182218Z","shell.execute_reply":"2026-02-23T16:03:47.207449Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def estimate_loss(model):\n    out = {}\n    model.eval()\n    with torch.inference_mode():\n        for split in ['train', 'val']:\n            losses = torch.zeros(eval_iters)\n            for k in range(eval_iters):\n                X, Y = get_batch(split)\n                with ctx:\n                    logits, loss = model(X, Y)\n                losses[k] = loss.item()\n            out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T16:03:51.707405Z","iopub.execute_input":"2026-02-23T16:03:51.707707Z","iopub.status.idle":"2026-02-23T16:03:51.713079Z","shell.execute_reply.started":"2026-02-23T16:03:51.707685Z","shell.execute_reply":"2026-02-23T16:03:51.712243Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Training Config\nimport torch\nfrom contextlib import nullcontext\n\nlearning_rate = 1e-4 \nmax_iters = 30000 \nwarmup_steps = 1000\nmin_lr = 1e-5 \neval_iters = 250\nbatch_size = 8 \nblock_size = 256 \n\ngradient_accumulation_steps = 128 \n\ndevice =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n# note: float16 data type will automatically use a GradScaler\n\n# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\ntorch.set_default_device(device)\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T16:03:53.370347Z","iopub.execute_input":"2026-02-23T16:03:53.370658Z","iopub.status.idle":"2026-02-23T16:03:53.379888Z","shell.execute_reply.started":"2026-02-23T16:03:53.370633Z","shell.execute_reply":"2026-02-23T16:03:53.379230Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7d7085cc8710>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import os\nfrom tqdm.auto import tqdm\n\n\nresume_path = \"/kaggle/working/Saved_A3000.pt\"  \n\nstart_epoch = 3000\nbest_val_loss = float(\"inf\")\ntrain_loss_list = []\nvalidation_loss_list = []\n\nprint(torch.cuda.current_device())\nprint(torch.cuda.get_device_name())\n\nif os.path.exists(resume_path):\n    print(\"Loading checkpoint:\", resume_path)\n\n    checkpoint = torch.load(resume_path, map_location=device)\n\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    \n    for state in optimizer.state.values():\n        for k, v in state.items():\n            if torch.is_tensor(v):\n                state[k] = v.to(device)\n\n    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n    scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n\n    best_val_loss = checkpoint[\"best_val_loss\"]\n\n    # load loss history if exists\n    if \"train_loss_list\" in checkpoint:\n        train_loss_list = checkpoint[\"train_loss_list\"]\n        validation_loss_list = checkpoint[\"validation_loss_list\"]\n\n    print(f\"Resuming from epoch {start_epoch}\")\n\nelse:\n    print(\"No checkpoint found. Starting fresh.\")\n\n\nmodel = model.to(device)\n\nprogress_bar = tqdm(range(start_epoch, max_iters), desc=\"Training\")\n\nfor epoch in progress_bar:\n\n    # --------------------\n    # TRAIN STEP\n    # --------------------\n    X, y = get_batch(\"train\")\n    X, y = X.to(device), y.to(device)\n\n    with ctx:\n        logits, loss = model(X, y)\n        loss = loss / gradient_accumulation_steps\n        scaler.scale(loss).backward()\n\n    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad(set_to_none=True)\n\n    scheduler.step()\n\n    # --------------------\n    # EVAL EVERY 100\n    # --------------------\n    if (epoch + 1) % 250 == 0:\n\n        losses = estimate_loss(model)\n\n        train_loss = losses[\"train\"].item()\n        val_loss   = losses[\"val\"].item()\n        current_lr = optimizer.param_groups[0][\"lr\"]\n\n        train_loss_list.append(train_loss)\n        validation_loss_list.append(val_loss)\n\n        # Update tqdm postfix\n        progress_bar.set_postfix(\n            train=f\"{train_loss:.4f}\",\n            val=f\"{val_loss:.4f}\",\n            lr=f\"{current_lr:.6f}\"\n        )\n\n        # Print stacked logs BELOW progress bar\n        tqdm.write(f\"Epoch {epoch+1}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n        tqdm.write(f\"The current learning rate: {current_lr:.5f}\")\n\n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), \"best_model_params.pt\")\n\n    # --------------------\n    # SAVE EVERY 1000\n    # --------------------\n    if (epoch + 1) % 1000 == 0:\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n                \"scheduler_state_dict\": scheduler.state_dict(),\n                \"scaler_state_dict\": scaler.state_dict(),\n                \"best_val_loss\": best_val_loss,\n                \"train_loss_list\": train_loss_list,\n                \"validation_loss_list\": validation_loss_list,\n            },\n            f\"Saved_A{epoch+1}.pt\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T16:08:22.198858Z","iopub.execute_input":"2026-02-23T16:08:22.199556Z","iopub.status.idle":"2026-02-23T17:09:11.682526Z","shell.execute_reply.started":"2026-02-23T16:08:22.199521Z","shell.execute_reply":"2026-02-23T17:09:11.681560Z"}},"outputs":[{"name":"stdout","text":"0\nTesla T4\nLoading checkpoint: /kaggle/working/Saved_A3000.pt\nResuming from epoch 3000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/27000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89ed9efe441d4ebab5854cb32cae2734"}},"metadata":{}},{"name":"stdout","text":"Epoch 3250: train loss 5.1947, val loss 5.1933\nThe current learning rate: 0.00020\nEpoch 3500: train loss 4.9828, val loss 4.9693\nThe current learning rate: 0.00021\nEpoch 3750: train loss 4.7201, val loss 4.7110\nThe current learning rate: 0.00022\nEpoch 4000: train loss 4.5607, val loss 4.5557\nThe current learning rate: 0.00023\nEpoch 4250: train loss 4.4610, val loss 4.4792\nThe current learning rate: 0.00025\nEpoch 4500: train loss 4.3925, val loss 4.3782\nThe current learning rate: 0.00026\nEpoch 4750: train loss 4.2872, val loss 4.3059\nThe current learning rate: 0.00027\nEpoch 5000: train loss 4.2450, val loss 4.2332\nThe current learning rate: 0.00028\nEpoch 5250: train loss 4.1821, val loss 4.2014\nThe current learning rate: 0.00029\nEpoch 5500: train loss 4.1008, val loss 4.1542\nThe current learning rate: 0.00030\nEpoch 5750: train loss 4.0957, val loss 4.1132\nThe current learning rate: 0.00031\nEpoch 6000: train loss 4.0517, val loss 4.1004\nThe current learning rate: 0.00032\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3688263067.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \"\"\"\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             return handle_torch_function(\n\u001b[0m\u001b[1;32m    617\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0;31m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_pop_mode_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1728\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n\n##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\noptimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n\nscheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\nscheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\nscheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n\n# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T16:03:59.034841Z","iopub.execute_input":"2026-02-23T16:03:59.035422Z","iopub.status.idle":"2026-02-23T16:03:59.042897Z","shell.execute_reply.started":"2026-02-23T16:03:59.035393Z","shell.execute_reply":"2026-02-23T16:03:59.042092Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/2132813893.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport tiktoken\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nenc = tiktoken.get_encoding(\"gpt2\")\n\n@torch.no_grad()\ndef generate_text(model, prompt, max_new_tokens=150, temperature=0.9, top_k=40):\n    model.eval()\n\n    ids = enc.encode(prompt)\n    ids = ids[-model.config.block_size:]  # crop to context window\n    x = torch.tensor(ids, dtype=torch.long, device=device)[None, :]\n\n    for _ in range(max_new_tokens):\n        x_cond = x[:, -model.config.block_size:]\n\n        with ctx:  # keep autocast if you used AMP in training\n            logits, _ = model(x_cond)\n\n        logits = logits[:, -1, :] / temperature\n\n        if top_k is not None:\n            v, _ = torch.topk(logits, top_k)\n            logits[logits < v[:, [-1]]] = -float(\"inf\")\n\n        probs = torch.softmax(logits, dim=-1)\n        next_id = torch.multinomial(probs, num_samples=1)\n\n        x = torch.cat([x, next_id], dim=1)\n\n    return enc.decode(x[0].tolist())\n\n\ndef Generate(ckpt_path, prompt, max_new_tokens=150, temperature=0.9, top_k=40):\n    ckpt = torch.load(ckpt_path, map_location=device)\n\n    model_template = GPT(config)\n    model = model_template.to(device) \n    model.load_state_dict(ckpt[\"model_state_dict\"])\n    model.eval()\n\n    return generate_text(model, prompt, max_new_tokens, temperature, top_k)\n\nprompt_list = [\"A farmer has 17 sheep. All but 9 run away. How many sheep does he have left? Explain your reasoning.\",\n              \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Why or why not?\",\n              \"Tom is twice as old as Anna. Five years ago, he was three times as old. How old are they now?\",\n              \"Plan a 7-day schedule to learn Python from scratch while working a full-time job.\",\n              \"Write a Python function to check if a string is a palindrome without using built-in reverse functions.\",\n              \"Write a function that finds the second largest number in a list. Handle duplicates properly.\",\n              \"Why does this code throw an error? nums = [1,2,3] print(nums[3])\",\n              \"Implement binary search in Python and explain its time complexity.\",\n              \"Given a list of integers, return the element with the highest absolute value. If there’s a tie, return the positive one.\",\n              \"Continue this story consistently for at least 300 words: A scientist discovered a door that appears only once every 10 years…\",\n              \"Write a short story where a red umbrella introduced in the first paragraph plays a critical role in the ending.\"\n              \"Explain how a computer boots up, step by step, in simple language.\",\n              \"Explain recursion in exactly 5 bullet points.\"\n              \"Write a short paragraph about the ocean without using the letter “e”.\",\n              \"Give me: 1. Definition of overfitting 2. One example 3. One prevention technique.\",\n              \"Who was Ada Lovelace and why is she important in computing history?\",\n              \"Explain how gradient descent works, including learning rate and local minima.\",\n              \"Describe “light” in three different contexts.\",\n              \"Can a triangle have two right angles? Why or why not?\",\n              \"I want to create a new folder on Windows using Command Prompt. Give step-by-step instructions.\"]\n\n\n\nModel_30000  = \"/kaggle/working/Saved_A1000.pt\"\n#Model_10000 = \"/kaggle/input/datasets/majorlakshya/dropout-updated/checkpoint_epoch_10000.pt\"\n#Model_15000 = \"/kaggle/input/datasets/majorlakshya/dropout-updated/checkpoint_epoch_15000 (1).pt\"\n#Model_20000 = \"/kaggle/input/datasets/majorlakshya/dropout-updated/checkpoint_epoch_20000.pt\"\n\n#output_30000  = Generate(Model_30000,  prompt, 150, 0.9, 40)\n#output_10000 = Generate(Model_10000, prompt, 150, 0.9, 40)\n#output_15000 = Generate(Model_15000, prompt, 150, 0.9, 40)\n#output_20000 = Generate(Model_20000, prompt, 150, 0.9, 40)\n\n#print(\"-----------------------------\")\n#print(\"30000 Epochs\\n\", output_30000)\n#print(\"-----------------------------\\n\")\n#print()\n#print(\"-----------------------------\")\n#print(\"10000 Epochs\\n\", output_10000)\n#print(\"-----------------------------\\n\")\n#print()\n#print(\"-----------------------------\")\n#print(\"15000 Epochs\\n\", output_15000)\n#print(\"-----------------------------\\n\")\n#print()\n#print(\"-----------------------------\")\n#print(\"20000 Epochs\\n\", output_20000)\n#print(\"-----------------------------\")\n\nfor i in prompt_list:\n    print(\"-------------------------------------------------\")\n    print(i)\n    output = Generate(Model_30000,i, 300, 0.9, 40)\n    print(output)\n    print(\"-------------------------------------------------\")\n    print()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T14:18:13.955240Z","iopub.execute_input":"2026-02-23T14:18:13.955856Z","iopub.status.idle":"2026-02-23T14:18:31.114683Z","shell.execute_reply.started":"2026-02-23T14:18:13.955824Z","shell.execute_reply":"2026-02-23T14:18:31.113494Z"}},"outputs":[{"name":"stdout","text":"-------------------------------------------------\nA farmer has 17 sheep. All but 9 run away. How many sheep does he have left? Explain your reasoning.\nA farmer has 17 sheep. All but 9 run away. How many sheep does he have left? Explain your reasoning.\n2. Be sure to know what this is.\n2. Are he as an actor or manager?\n2. See this: What do you think about the same thing about? How much good? It means that the man are not a good woman, he will probably not have a better chance to play him on the other.\n9.\n1. It’s the next thing to be able to do so if he is to play one of the best-time players if he is in the country’s first-time. He's not sure who you are at the same time. He will still have the same number of kids to move and put into them around the next year.\n3.\n2. Don’t see if you want to win the team.\n3. How long ago he has to do that?\n2. The next two days of getting out of a team will be able to be a team of player.\n5. Try to know he needs to do.\n3. Did he take an idea?\n3. How many of his games is he?\n4.\n3. Take the job for the match to the game, but when it means you're looking for the game and you will be a player. It's hard as much money.\n2. If you are all time, then do the team do the team on the team last year?\n3. Not only if you use a\n-------------------------------------------------\n\n-------------------------------------------------\nIf all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Why or why not?\nIf all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Why or why not?\nIf she gets to that, she has been with so much money she’s used to make it, we are so happy to have her time to put her to kill her. She has no plans to get it to the ground and be aware that he is trying to go to her.\n\nHe just wants to be there to be happy by having any money or even a job.\n\nHe knows he would have to be a good person if she should want to stay in it, and then the other thing that he’ll want to be ready to be out, just to find an opportunity to give him a good plan and do whatever that he had in.\nBough: Maybe we can’t make a mistake. Let’s look at how much money can and you get to get to the right time and the money is at all.\nHe could say nothing to do with.\n\nRigg: I guess what kind of data-made computer-and-to-one technology system has been a better source in a way. We haven’t yet to do something that the project is not the biggest problem that we do.\nRawn: The system can change the number of parameters of different languages, but it’s not clear how much you don’t have a great way. It is like a lot of people, but not even if they are able to do, they do very well.\nWeb\n-------------------------------------------------\n\n-------------------------------------------------\nTom is twice as old as Anna. Five years ago, he was three times as old. How old are they now?\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2517283099.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel_30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2517283099.py\u001b[0m in \u001b[0;36mGenerate\u001b[0;34m(ckpt_path, prompt, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m prompt_list = [\"A farmer has 17 sheep. All but 9 run away. How many sheep does he have left? Explain your reasoning.\",\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2517283099.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, prompt, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# keep autocast if you used AMP in training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/4040496400.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/4040496400.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/4040496400.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13}]}